---
title: "Python Puzzles in the Age of AI: Do We Still Need to Solve Them?"
date: 2025-01-28
tags: [Python, AI, Career, Testing, Interviews]
excerpt: "Ten years of Python experience, two years of AI coding assistants, and one humbling coding interview. Are we testing the right skills anymore?"
author: Quality Playbook
---

I've been writing Python almost daily for a decade. Started back in the Python 2.x days. Built test frameworks, automation tools, monitoring systems, CI/CD pipelines—you name it. In my recent leadership roles, Python has been my Swiss Army knife for solving problems.

Now I'm interviewing for both leadership and IC roles. For the IC positions, I'm targeting SDET roles at companies building test automation with Python. These are roles where I know I can add real value.

## The Interview That Humbled Me

Recently, I had my first-ever live coding assessment. Two Python data structure puzzles. Should be straightforward, right? Ten years of experience, surely I've got this.

I didn't.

I got maybe 80% of the way there, but I choked. The pressure of the live coding environment got to me. I stumbled on problems that, given 15 minutes alone with my editor, I could have solved easily. Unsurprisingly, I didn't get the job.

## The AI Elephant in the Room

Here's the thing that keeps nagging at me: For the last two years, I've been using Copilot, Cursor, and most recently, Claude Code. Every. Single. Day. These AI coding assistants aren't just autocomplete—they're legitimate productivity multipliers.

Coding agents are here. They're real. They're transforming how we work.

So I have to ask: **Do we really need to solve data structure puzzles anymore?**

## The Disconnect

I get it. Coding assessments are standard for IC roles. You need to validate that someone can actually write code. Fair enough.

But there's a massive disconnect between how we interview and how we actually work:

- **In interviews:** Solve this binary tree traversal problem in 45 minutes with someone watching you.
- **In real work:** Build a test automation framework that scales across 50 microservices, integrates with your CI/CD pipeline, provides actionable failure analysis, and doesn't become technical debt in six months.

Which one requires deeper engineering skill?

## What Are We Actually Testing?

When you ask someone to solve a data structure puzzle in a live coding session, what are you really evaluating?

- **Memorization?** How many LeetCode problems they've practiced?
- **Pressure performance?** Their ability to think while being watched?
- **Academic knowledge?** Their CS fundamentals?

What you're probably *not* testing:

- Can they architect a maintainable test framework?
- Can they debug a flaky test that fails 2% of the time?
- Can they effectively use modern tools (including AI) to be productive?
- Can they write code that their teammates will thank them for?

## The Honest Truth About Real Work

In my day-to-day work, I:

- Use AI assistants to handle boilerplate and routine patterns
- Focus my brain on architecture, design decisions, and problem-solving
- Spend more time reviewing code than writing it from scratch
- Collaborate with teammates to solve complex problems
- Read documentation, Stack Overflow, and GitHub issues constantly

None of this happens in a vacuum, without references, or under artificial time pressure.

## Maybe I'm Just Salty

Look, I failed the interview. Maybe I'm just rationalizing. Maybe I should have practiced more LeetCode. Maybe the ability to solve these puzzles does correlate with being a good engineer.

But I've hired and managed teams of SDETs. I've seen who succeeds and who struggles. And I've never once thought, "If only this person could reverse a linked list faster."

## The Skills That Actually Matter

For SDET roles specifically, here's what I think matters:

1. **System thinking** - Understanding how tests fit into the broader engineering ecosystem
2. **Tool building** - Creating frameworks that make the team more productive
3. **Debugging tenacity** - Finding that one-in-a-thousand race condition
4. **Code quality** - Writing tests that are more maintainable than the code they test
5. **Pragmatism** - Knowing when to automate and when not to
6. **Adaptability** - Learning new tools, frameworks, and yes, AI assistants

Can you assess these in a 45-minute coding puzzle? Probably not.

## What Should We Do Instead?

I don't have all the answers. But here are some ideas:

- **Take-home projects** - Give candidates a realistic problem, let them use their normal tools (including AI), and evaluate the result
- **Pair programming** - Work together on a real problem from your codebase
- **Architecture discussions** - Review their past work and discuss design decisions
- **Code review exercises** - See how they evaluate and improve existing code
- **Tool evaluation** - Watch them debug a failing test or investigate a production issue

These still validate coding ability, but in contexts that mirror actual work.

## The Bottom Line

AI coding assistants aren't going away. They're getting better, faster, and more integrated into our workflows. Developers who effectively leverage these tools are more productive, not less skilled.

Maybe it's time our interview processes caught up with how we actually build software in 2025.

Or maybe I just need to practice my tree traversals.

---

*What do you think? Are coding puzzles still relevant? How do you evaluate engineering skill in the age of AI? I'd love to hear your perspective—find me on [LinkedIn](https://www.linkedin.com/in/tomdrake-qe) or share your thoughts.*
